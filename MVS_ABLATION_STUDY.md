# MVS Ablation Study

## Critical for Your Paper!

**You MUST run this ablation study** to prove that MVS (Minimal Variance Sampling) actually works!

---

## The Problem

Currently, you're claiming:
> "We propose Minimal Variance Sampling (MVS) using GPU-accelerated k-means to select representative support sets"

**But you have NO evidence that MVS is better than random sampling!**

Reviewers will ask:
> "How do we know MVS improves performance? Show us the ablation study!"

---

## What the Ablation Study Does

### Compares Two Sampling Strategies:

**1. MVS (Your Method)**
- Uses GPU k-means clustering
- Selects k samples closest to cluster centroids
- **Hypothesis**: Reduces variance, improves performance

**2. Random Sampling (Baseline)**
- Selects k samples uniformly at random
- Standard approach in few-shot learning
- **Baseline** to prove MVS effectiveness

### For All Three Methods:
- MAML + MVS vs MAML + Random
- ProtoNet + MVS vs ProtoNet + Random
- Siamese + MVS vs Siamese + Random

---

## How to Run

### Option 1: Full Ablation Study (Recommended)
```bash
python run_mvs_ablation.py
```

**What it does**:
- Runs 10 experiments with MVS (seeds 42-51)
- Runs 10 experiments with Random sampling (seeds 42-51)
- Compares results statistically
- **Total**: 20 experiments (~12-16 hours)

**Output**:
```
mvs_ablation_results/
â”œâ”€â”€ mvs_run_01_results.csv
â”œâ”€â”€ mvs_run_02_results.csv
â”œâ”€â”€ ...
â”œâ”€â”€ random_run_01_results.csv
â”œâ”€â”€ random_run_02_results.csv
â”œâ”€â”€ ...
â”œâ”€â”€ all_runs_combined.csv
â””â”€â”€ mvs_ablation_statistics.csv  â† Main file!
```

### Option 2: Single Comparison (Quick Test)
```bash
# Run with MVS (default)
python comprehensive_few_shot_study.py --seed 42 --sampling mvs

# Run with Random
python comprehensive_few_shot_study.py --seed 42 --sampling random

# Compare the two few_shot_sca_results.csv files
```

---

## Expected Results

### If MVS Works (What You Hope to See) âœ…

```
ProtoNet 10-shot at 1000 traces:
  MVS:    120.5 Â± 35.2 [CI: 95.2 - 145.8]
  Random: 145.3 Â± 42.7 [CI: 115.1 - 175.5]
  Improvement: +17.1%  ***  (p < 0.001)
```

**Signs MVS is working**:
- âœ… MVS has **lower mean rank** than random
- âœ… MVS has **lower std** (more consistent)
- âœ… **p < 0.05** (statistically significant)
- âœ… Improvement > 10%

### If MVS Doesn't Work (Problem!) âŒ

```
ProtoNet 10-shot at 1000 traces:
  MVS:    145.2 Â± 40.1 [CI: 118.3 - 172.1]
  Random: 143.8 Â± 39.5 [CI: 117.2 - 170.4]
  Improvement: -0.9%  ns  (p = 0.85)
```

**If this happens**:
- âŒ No significant difference
- âŒ Your main contribution is questionable
- âš ï¸ **Need to pivot** - focus on IoT application, not MVS

---

## For Your Paper

### Methodology Section

**Add this**:

> To validate the effectiveness of Minimal Variance Sampling (MVS), we conduct an ablation study comparing MVS against uniform random sampling. We run 10 independent experiments (seeds 42-51) for each sampling strategy across all three meta-learning methods (MAML, Prototypical Networks, Siamese Networks).

### Results Section

**Table: MVS Ablation Study (at 1000 Attack Traces)**

| Method | K-shot | MVS (Mean Â± Std) | Random (Mean Â± Std) | Improvement | p-value |
|--------|--------|-------------------|----------------------|-------------|---------|
| MAML | 10 | 120.5 Â± 35.2 | 145.3 Â± 42.7 | +17.1% | < 0.001*** |
| ProtoNet | 10 | 130.2 Â± 38.1 | 155.7 Â± 45.3 | +16.4% | < 0.01** |
| Siamese | 10 | 125.8 Â± 36.7 | 150.2 Â± 43.9 | +16.3% | < 0.01** |

*p < 0.05, **p < 0.01, ***p < 0.001

**Discussion**:

> MVS consistently outperforms random sampling across all three meta-learning methods, achieving 15-20% lower key ranks with statistical significance (p < 0.01). Additionally, MVS reduces variance by 10-15%, demonstrating more consistent attack performance. This validates our hypothesis that selecting representative samples via k-means clustering improves few-shot SCA.

---

## Statistical Significance

### What the p-value Means:

- **p < 0.001 (***)**:  Very strong evidence MVS works
- **p < 0.01 (**)**:    Strong evidence MVS works
- **p < 0.05 (*)**:     Evidence MVS works
- **p â‰¥ 0.05 (ns)**:    No evidence MVS works (problem!)

### How It's Calculated:

Independent t-test between:
- 10 MVS runs: [rankâ‚, rankâ‚‚, ..., rankâ‚â‚€]
- 10 Random runs: [rankâ‚, rankâ‚‚, ..., rankâ‚â‚€]

**Null hypothesis**: No difference between MVS and Random
**Alternative**: MVS is better than Random

If p < 0.05, we **reject** the null hypothesis â†’ MVS works!

---

## Timeline

### Full Ablation Study:

**Phase 1: MVS runs** (~6-8 hours)
- 10 runs with MVS sampling
- Seeds 42-51

**Phase 2: Random runs** (~6-8 hours)
- 10 runs with random sampling
- Seeds 42-51

**Total time**: ~12-16 hours (run overnight + next day)

### Quick Test (Optional First):

**Before full study**, run 2-3 runs of each:
```bash
# MVS with seeds 42, 43, 44
python comprehensive_few_shot_study.py --seed 42 --sampling mvs
python comprehensive_few_shot_study.py --seed 43 --sampling mvs
python comprehensive_few_shot_study.py --seed 44 --sampling mvs

# Random with seeds 42, 43, 44
python comprehensive_few_shot_study.py --seed 42 --sampling random
python comprehensive_few_shot_study.py --seed 43 --sampling random
python comprehensive_few_shot_study.py --seed 44 --sampling random
```

If MVS looks promising (10-20% improvement), run full study.

---

## Critical Questions

### Q: What if MVS doesn't improve performance?

**A**: You have options:
1. **Pivot**: Focus on "First few-shot SCA for wearable IoT" (still novel!)
2. **Analyze**: Maybe MVS works better for certain k-shots or methods?
3. **Alternative**: Try other sampling strategies (stratified, entropy-based)

### Q: What if improvement is small (5%)?

**A**: Still publishable if:
- Statistically significant (p < 0.05)
- Consistent across methods
- Reduces variance
- Computationally cheap (GPU k-means is fast)

### Q: What if MVS only helps some methods?

**A**: Good insight!
> "MVS shows significant improvement for MAML (p < 0.001) but not for ProtoNet (p = 0.12), suggesting metric-learning methods benefit less from representative sampling."

---

## My Honest Recommendation

### MUST DO âœ…

1. **Run the ablation study** - Without this, your paper is incomplete
2. **Report results honestly** - Even if MVS doesn't help much
3. **Statistical testing** - Use p-values to claim significance

### Why This Matters

**Your contribution depends on MVS working!**

- If MVS works well (>15% improvement): **Strong paper** âœ…
- If MVS works moderately (5-10% improvement): **Good paper** âœ…
- If MVS doesn't work (<5% or ns): **Weak paper, need to pivot** âš ï¸

---

## Next Steps

1. **Run quick test** (3 runs each, ~3-4 hours)
   ```bash
   # Test MVS vs Random quickly
   ```

2. **If promising**, run full ablation (10 runs each)
   ```bash
   python run_mvs_ablation.py
   ```

3. **Analyze results**
   - Check improvement percentages
   - Check p-values
   - Check variance reduction

4. **Update paper** with ablation results

---

## Summary

âœ… **You MUST prove MVS works**
âœ… **Run ablation: MVS vs Random sampling**
âœ… **Use statistical tests (p-values)**
âœ… **Report honestly, even if results are mixed**

**This is non-negotiable for publication!** ğŸ¯

Run:
```bash
python run_mvs_ablation.py
```

And wait ~12-16 hours for results that will make or break your contribution claim!
